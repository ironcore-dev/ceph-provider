{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ceph Provider","text":"<p>The <code>ceph-provider</code> project is a Ceph based provider implementation of the ironcore types</p> <pre><code>graph TD\n    ceph-provider -. implements .-&gt; storage.ironcore.dev</code></pre> <p>Namely <code>ceph-provider</code> implements the <code>Volume</code>  and <code>VolumePool</code> types.  Additionally, it announces the available <code>VolumeClasses</code> which are supported by the <code>VolumePool</code> based on configured criteria.</p> <p>Further information about the architecture and concepts of the <code>ceph-provider</code> project can be found in the  architecture section.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>This section covers the core concepts of the <code>ceph-provider</code> project.</p> <p>The <code>ceph-provider</code> is an implementor of the <code>ironcore runtime interface</code> (<code>IRI</code>) for <code>Volumes</code> and <code>Buckets</code>. It consists of the <code>ceph-volume-provider</code> and <code>ceph-bucket-provider</code> in order to implement the VolumeRuntime respectively the BucketRuntime.</p> <p>A <code>ceph-provider</code> is usually deployed along with a poollet. A poollet resolves dependencies, e.g. an encryption secret, and calls with the consolidated resource the <code>ceph-provider</code>.  The <code>ceph-provider</code> persists the required state and reconciles the resource in an asynchronous manner. </p>"},{"location":"architecture/#ceph-volume-provider","title":"ceph-volume-provider","text":"<p>The <code>ceph-volume-provider</code> interacts directly with a defined <code>ceph cluster</code>.  A <code>Volume</code> is provisioned by creating a <code>ceph image</code>. If needed, an image is created with a pre-defined <code>os image</code>.</p> <p>The following diagram visualizes the interplay of the different components:  <pre><code>graph TD\n    C([ceph-volume-provider])\n    P([volumepoollet])\n\n    P -- announces --&gt; VP[VolumePool]\n    P -- watches --&gt; V[Volumes]\n\n    P -- uses IRI --&gt; C\n\n    C -- creates --&gt; I[Ceph Image]\n    C -- defines --&gt; VC[Supported VolumeClasses]</code></pre></p>"},{"location":"architecture/#ceph-bucket-provider","title":"ceph-bucket-provider","text":"<p>The <code>ceph-bucket-provider</code> utilizes <code>rook</code> CRD's to back the ironcore <code>Bucket</code> resource. Rook ensures that a <code>ObjectBucketClaim</code> (and an access secret) is being reconciled. </p> <p>The following diagram visualizes the interplay of the different components: <pre><code>graph TD\n    C([ceph-bucket-provider])\n    P([bucketpoollet])\n    R([rook])\n\n    P -- announces --&gt; BP[BucketPool]\n    P -- watches --&gt; B[Buckets]\n\n    P -- uses IRI --&gt; C\n\n    C -- defines --&gt; VC[Supported BucketClasses]\n    C -- creates --&gt; OBC[ObjectBucketClaim]\n    R -- reconciles --&gt; OBC</code></pre></p>"},{"location":"development/setup/","title":"Local Development Setup","text":""},{"location":"development/setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>go &gt;= 1.19</li> <li><code>git</code>, <code>make</code> and <code>kubectl</code></li> <li>Kustomize</li> <li>Minikube or a real cluster</li> </ul>"},{"location":"development/setup/#preperation","title":"Preperation","text":""},{"location":"development/setup/#setup-ceph-cluster","title":"Setup Ceph Cluster","text":"<p>Reference:  rook docs</p>"},{"location":"development/setup/#install-cert-manager","title":"Install cert-manager","text":"<p>If there is no cert-manager present in the cluster it needs to be installed.</p> <pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.8.0/cert-manager.yaml\n</code></pre>"},{"location":"development/setup/#setup-ironcore","title":"Setup <code>ironcore</code>","text":"<p>Reference: ironcore docs</p>"},{"location":"development/setup/#setup-rook","title":"Setup <code>Rook</code>","text":"<ol> <li> <p>Install Rook operator and <code>CRD</code>s <pre><code>kubectl apply -k ./rook\n</code></pre></p> </li> <li> <p>Verify the rook-ceph-operator is in the Running state before proceeding  <pre><code>kubectl -n rook-ceph get pod\n</code></pre></p> </li> <li> <p>Create a Rook Ceph Cluster see: Rook Docs</p> </li> <li> <p>Verify cluster installation. List all rook pods again:  <pre><code>kubectl -n rook-ceph get pod\n</code></pre> In the end you should see all pods <code>Running</code> or <code>Completed</code> and have at least one <code>rook-ceph-osd-*</code> Pod: <pre><code>NAME                                            READY   STATUS      RESTARTS   AGE\ncsi-cephfsplugin-b7ktv                          3/3     Running     0          63d\ncsi-cephfsplugin-provisioner-59499cbcdd-wvnfq   6/6     Running     0          63d\ncsi-rbdplugin-bs4tn                             3/3     Running     6          63d\ncsi-rbdplugin-provisioner-857d65496c-mxjp4      6/6     Running     0          63d\nrook-ceph-mgr-a-769964c967-9kmxq                1/1     Running     0          26d\nrook-ceph-mon-a-66b5cfc47f-8d4ts                1/1     Running     0          63d\nrook-ceph-operator-75c6d6bbfc-b9q9n             1/1     Running     0          63d\nrook-ceph-osd-0-7464fbbd49-szdrp                1/1     Running     0          63d\nrook-ceph-osd-prepare-minikube-7t4mk            0/1     Completed   0          6d8h\n</code></pre></p> </li> <li> <p>Deploy a <code>CephCluster</code> <pre><code>kubectl apply -f ./rook/cluster.yaml\n</code></pre> Ensure that the cluster is in <code>Ready</code> phase</p> </li> </ol> <pre><code>kubectl get cephcluster -A\n</code></pre> <ol> <li>Deploy a <code>CephBlockPool</code>, <code>CephObjectStore</code> &amp; <code>StorageClass</code> <pre><code>kubectl apply -f ./rook/pool.yaml\n</code></pre></li> </ol>"},{"location":"development/setup/#clone-the-repository","title":"Clone the Repository","text":"<p>To bring up and start locally the <code>ceph-provider</code> project for development purposes you first need to clone the repository.</p> <pre><code>git clone git@github.com:ironcore-dev/ceph-provider.git\ncd ceph-provider\n</code></pre>"},{"location":"development/setup/#build-the-ceph-provider","title":"Build the <code>ceph-provider</code>","text":"<ol> <li> <p>Build the <code>ceph-volume-provider</code> <pre><code>make build-volume\n</code></pre></p> </li> <li> <p>Build the <code>ceph-bucket-provider</code> <pre><code>make build-bucket\n</code></pre></p> </li> </ol>"},{"location":"development/setup/#run-the-ceph-volume-provider","title":"Run the <code>ceph-volume-provider</code>","text":"<p>The required <code>ceph-provider</code> flags needs to be defined in order to connect to ceph. </p> <p>The following command starts a <code>ceph-volume-provider</code> and connects to a local <code>ceph</code> cluster. <pre><code>go run ./cmd/volumeprovider/main.go \\\n    --address=./iri-volume.sock\n    --supported-volume-classes=./classes.json\n    --zap-log-level=2\n    --ceph-key-file=./key\n    --ceph-monitors=192.168.64.23:6789\n    --ceph-user=admin\n    --ceph-pool=ceph-provider-pool\n    --ceph-client=client.ceph-provider-pool\n</code></pre></p> <p>Sample <code>supported-volume-classes.json</code> file:  <pre><code>[\n  {\n    \"name\": \"experimental\",\n    \"capabilities\": {\n      \"tps\": 262144000,\n      \"iops\": 15000\n    }\n  }\n]\n</code></pre></p> <p>The <code>ceph key</code> can be retrieved from the keyring by decoding (base64) the keyring and using only the <code>key</code>.  <pre><code>kubectl get secrets -n rook-ceph rook-ceph-admin-keyring -o yaml\n</code></pre></p>"},{"location":"development/setup/#run-the-ceph-bucket-provider","title":"Run the <code>ceph-bucket-provider</code>","text":"<p>The required <code>ceph-provider</code> flags needs to be defined in order to work with rook.</p> <p>The following command starts a <code>ceph-bucket-provider</code>.  The flag <code>bucket-pool-storage-class-name</code> defines the <code>StorageClass</code> and hereby implicit the <code>CephBlockPool</code> (see rook docs).  <pre><code>go run ./cmd/bucketprovider/main.go \\\n    --address=./iri-bucket.sock\n    --bucket-pool-storage-class-name=rook-ceph-bucket\n</code></pre></p>"},{"location":"development/setup/#interact-with-the-ceph-provider","title":"Interact with the  <code>ceph-provider</code>","text":""},{"location":"development/setup/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>irictl-volume<ul> <li>locally running or</li> <li>https://github.com/ironcore-dev/ironcore/pkgs/container/ironcore-irictl-volume</li> </ul> </li> <li>irictl-bucket<ul> <li>locally running or</li> <li>https://github.com/ironcore-dev/ironcore/pkgs/container/ironcore-irictl-bucket</li> </ul> </li> </ul>"},{"location":"development/setup/#listing-supported-volumeclass","title":"Listing supported <code>VolumeClass</code>","text":"<pre><code>irictl-volume --address=unix:./iri-volume.sock get volumeclass\nName           TPS         IOPS\nexperimental   262144000   15000\n</code></pre>"},{"location":"development/setup/#listing-supported-volumeclass_1","title":"Listing supported <code>VolumeClass</code>","text":"<pre><code>irictl-volume --address=unix:./iri-volume.sock get volumeclass\nName           TPS         IOPS\nexperimental   262144000   15000\n</code></pre>"},{"location":"development/setup/#creating-a-volume","title":"Creating a  <code>Volume</code>","text":"<pre><code>irictl-volume --address=unix:./iri-volume.sock create volume -f ./volume.json\n\nCreated volume 796264618065bb31024ec509d4ed8a87ed098ee8e89b370c06b0522ba4bf1e2\n</code></pre> <p>Sample volume.json <pre><code>{\n  \"metadata\": {\n    \"labels\": {\n      \"test.api.ironcore.dev/volume-name\": \"test\"\n    }\n  },\n  \"spec\": {\n    \"class\":  \"experimental\",\n    \"resources\":  {\n      \"storage_bytes\": 10070703360\n    }\n  }\n}\n</code></pre></p>"},{"location":"development/setup/#listing-volumes","title":"Listing <code>Volume</code>s","text":"<pre><code>irictl-volume --address=unix:./iri-volume.sock get  volume\nID                                                                Class          Image   State              Age\n796264618065bb31024ec509d4ed8a87ed098ee8e89b370c06b0522ba4bf1e2   experimental           VOLUME_AVAILABLE   2s\n</code></pre>"},{"location":"development/setup/#deleting-a-volumes","title":"Deleting a <code>Volume</code>s","text":"<pre><code>irictl-volume --address=unix:./iri-volume.sock delete  volume 796264618065bb31024ec509d4ed8a87ed098ee8e89b370c06b0522ba4bf1e2\nVolume 796264618065bb31024ec509d4ed8a87ed098ee8e89b370c06b0522ba4bf1e2 deleted\n</code></pre>"},{"location":"usage/","title":"Usage Guides","text":"<p>This section provides an overview on how <code>Volume</code>s from the ironcore project can be provisioned using the <code>ceph-provider</code> provider. The samples are equivalent for <code>Bucket</code>s. </p>"},{"location":"usage/#available-pools-and-classes","title":"Available Pools and Classes","text":"<p>As a user you can request storage by creating a <code>Volume</code>. It will be allocated in the referenced <code>VolumePool</code>.  The <code>VolumeClasses</code> define the capabilities in terms of IOPS, BPS limits and other resource requirements. </p> <p>Get the available <code>VolumePools</code> with the corresponding <code>VolumeClasses</code></p> <pre><code>kubectl get volumeclasses \nNAME   AGE\nfast   4d18h\nslow   4d18h\n\nkubectl get volumepool\nNAME   VOLUMECLASSES   AGE\nceph   fast,slow       4d17h\n</code></pre>"},{"location":"usage/#creating-a-volume","title":"Creating a <code>Volume</code>","text":"<p>A <code>Volume</code> is referencing a <code>VolumePool</code> and a matching <code>VolumeClass</code> which the <code>VolumePool</code> supports.</p> <pre><code># sample-volume.yaml\napiVersion: storage.ironcore.dev/v1alpha1\nkind: Volume\nmetadata:\n  name: sample-volume\n  namespace: default\nspec:\n  volumeClassRef:\n    name: fast\n  volumePoolRef:\n    name: ceph\n  resources:\n    storage: 1Gi\n</code></pre> <pre><code>kubectl apply -f sample-volume.yaml \nvolume.storage.ironcore.dev/sample-volume created\n</code></pre>"},{"location":"usage/#volume-status","title":"<code>Volume</code> Status","text":"<p>Once the <code>Volume</code> is provisioned the state will change to <code>Available</code>.</p> <pre><code>kubectl get volumes\nNAMESPACE       NAME            VOLUMEPOOLREF   VOLUMECLASS   STATE       PHASE     AGE\ndefault   sample-volume   ceph            fast          Available   Unbound   4m1s\n</code></pre> <p>The status of the <code>Volume</code> will contain the information which is needed to be able to consume the volume with a ceph client.</p> <pre><code>apiVersion: storage.ironcore.dev/v1alpha1\nkind: Volume\nmetadata:\n  name: sample-volume\n  namespace: default\nspec:\n  ...\nstatus:\n  access:\n    driver: ceph\n    secretRef:\n      name: sample-volume\n    volumeAttributes:\n      WWN: f1243b9a192c4825\n      image: ceph/csi-vol-ae2bb4d0-2cf1-11ed-a7db-b6307c819ad0\n      monitors: '[2a10:afc0:e013:4030::]:6789'\n  lastPhaseTransitionTime: \"2022-09-05T08:05:48Z\"\n  phase: Unbound\n  state: Available\n</code></pre> <p>The <code>secretRef</code> in the status defines the <code>secret</code> with the  access credentials for the specific <code>Volume</code>.</p> <pre><code>kubectl get secrets\nNAME            TYPE     DATA   AGE\nsample-volume   Opaque   2      93s\n</code></pre>"}]}